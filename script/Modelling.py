# -*- coding: utf-8 -*-
"""ISE-540 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fH7w56n7ZqFVswZqTZP2tRY_n41vm1zk

# Web Scrapping
"""

import requests
import bs4
from bs4 import BeautifulSoup
import os

headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36 Edg/94.0.992.31'}
 
def getUrl(url):
  r = requests.get(url, headers=headers)
  r.encoding = r.apparent_encoding
  r.raise_for_status()
  return r.text

'''
Demo of URL about featured films between 1995 and 2020
'''
URL = 'https://www.imdb.com/search/keyword/?ref_=kw_ref_yr&mode=detail&page=&title_type=movie&sort=moviemeter,asc&release_date=1995%2C2000'

def get_page(URL = 'https://www.imdb.com/search/keyword/', page = 1, page_str = 'page='):
  '''
  URL: str - the string of URL deleting the explicit page number
  page: int - the page number
  page_str: str - the key word to test validity avoiding corner case

  Output: str - the targeted url of specific page
  '''
  # Change the page number of target url
  if page_str not in URL:
    print('Cannot find the targeted page keyword, please change another')
    return
  start_index = URL.find(page_str)
  end_index = start_index + len(page_str)
  URL = list(URL)
  URL.insert(end_index + 1, str(page))
  return ''.join(URL)

def get_max_page(URL, left_key_word = 'of', right_key_word = 'titles'):
  '''
  Objective: Getting the possible maximum page of current list
  URL: str - the string of URL of any page list
  left & right keywords: relating starting and ending index

  Output: int - the maximum page of current rank list
  '''
  html = getUrl(URL)
  bf = BeautifulSoup(html, 'html.parser')
  div_content = str(bf.find('div', attrs = {'class': 'desc'}))
  start_index = div_content.find(left_key_word)
  end_index = div_content.find(right_key_word)

  try:
    str_page =  div_content[start_index:end_index].split(' ')[1]
  except:
    str_page = '0'
  if ',' in str_page:
    left, right = str_page.split(',')
    return (int(left)*1000 + int(right)) // 50 + 1
  return int(str_page) // 50 + 1

def get_all_list_page(URL, page_str = 'page', left_key_word = 'of', right_key_word = 'titles'):
  '''
  URL: str - the string of URL deleting the explicit page number
  page_str: str - the key word to test validity avoiding corner case
  left & right keywords: relating starting and ending index

  Output: List[str] - the URL's array of all or Top 10,000 films page list
  '''
  first_page_url = get_page(URL, 1, page_str)
  max_page = get_max_page(first_page_url, left_key_word, right_key_word)
  
  if max_page > 200:
    print(f'The size of film list is over the allowable maximum, current url is {URL[-11:]}')
  
  size = min(max_page, 200)
  res = [] # The variable caching urls
  for page_i in range(size):
    res.append(get_page(URL, page_i + 1, page_str))
  
  return res

get_max_page('https://www.imdb.com/search/keyword/?ref_=kw_ref_yr&mode=detail&page=1&title_type=movie&user_rating=3.0%2C&num_votes=100%2C&sort=moviemeter,asc&release_date=2022%2C2022')

page_url_list = get_all_list_page(URL)

def get_all_film_url(page_url):
  '''
  page_url: List[str] - the url list of featured film pages
  
  Output: List[str] - the backend tag for url list of film url on one page
  '''
  html = getUrl(page_url)
  bf = BeautifulSoup(html, 'html.parser')
  temp = bf.find_all('h3', attrs = {'class':'lister-item-header'})
  linkList = []
  for list in temp:
    tempList=list.find('a', attrs = {'class':''})
    link = tempList.get('href')
    linkList.append(link)
  return linkList

# Get the full URL of film
def readable_film_url(back_tag = '', front_tag = 'https://www.imdb.com'):
  return front_tag + back_tag

url_1 = get_page(URL, 1)
film_list = get_all_film_url(url_1)

film_url = 'https://www.imdb.com/title/tt15791034/?ref_=kw_li_tt'
r = requests.get(film_url)
bf = BeautifulSoup(r.text, 'lxml')
details = bf.find('section',{'data-testid':'Details', 'class': 'ipc-page-section ipc-page-section--base celwidget'})

def get_film_info(film_url):
  '''
  film_url: str - the URL of specific film   
  Output: dict {key-value pairs} - including related info about specific film
  '''
  try:
    r = requests.get(film_url)
    bf = BeautifulSoup(r.text, 'lxml')
    bf_info = bf.find('section', {'class': 'ipc-page-section ipc-page-section--baseAlt ipc-page-section--tp-none ipc-page-section--bp-xs sc-7643a8e3-1 glXLDh'})
  except:
    print(f'request cannot get in {film_url}')
    return {
      'film_name': None,'synopsis': None,'genre_list': None,'publish_year': None,'MPAA': None,
      'Duration_minute': None,'Rating': None,'Rating_popularity': None,'Popularity': None,
      'Director': None,'Writer': None,'Stars': None,'Awards': None,'Release_date': None,
      'Country_of_origin': None,'Language': None,'Filming_locations': None,'Production_companies': None,
      'Budget': None,'Gross_US_Canada': None,'Opening_weekend': None,'Gross_worldwide': None,
      'Runtime': None,'Color': None,'Sound_mix': None,'Aspect_ratio': None,'film_url': film_url
  }

  # Get Film Name
  try:
    name_part = bf_info.find('div', attrs = {'class': 'sc-80d4314-1 fbQftq'}).find('h1')
    film_name = name_part.get_text()
  except:
    film_name = None
    print(f'No film name and current url is {film_url}')
  
  # Get Story Line of Film
  try:
    story_part = bf_info.find('div', attrs = {'class': 'sc-16ede01-7 hrgVKw', 'data-testid': 'plot'})
    story_line = story_part.find('span', {'data-testid': 'plot-xl'}).get_text()
  except:
    story_line = None
    print(f'No synopsis and current url is {film_url}')

  # Get genre tags of film
  try:
    genre_part = bf_info.find('div', attrs = {'class': 'ipc-chip-list--baseAlt ipc-chip-list sc-16ede01-5 ggbGKe', 'data-testid': 'genres'})
    genre_part = genre_part.find_all('a')
    genre_list = []
    for genre in genre_part:
      genre_list.append(genre.get_text())
  except:
    genre_list = None

  # Categorical bar
  category_part = bf_info.find('div', {'class':'sc-80d4314-2 iJtmbR'}).find_all('li')
  # Publish_year
  try:
    publish_year = category_part[0].find('span').get_text() # Publishing year
  except:
    publish_year = None
    print(f'No publish year and current url is {film_url}')
  # MPAA
  try:
    MPAA = category_part[1].find('span').get_text()# MPAA
  except:
    MPAA = None
  # Duration
  try:
    Duration_str = category_part[2].get_text() # Duration
  except:
    try:
      Duration_str = category_part[1].get_text()
    except:
      Duration_str = None

  try:
    Rating = bf_info.find('div', {'data-testid': 'hero-rating-bar__aggregate-rating__score', 'class': 'sc-7ab21ed2-2 kYEdvH'}).find('span').get_text()
    # Number of users rated the film
    Rating_popularity = bf_info.find('div', {'class': 'sc-7ab21ed2-3 dPVcnq'}).get_text()
  except:
    Rating = None
    Rating_popularity = None

  try:
    Popularity = bf.find('div', {"data-testid":"hero-rating-bar__popularity__score", 
                                 "class":"sc-edc76a2-1 gopMqI"}).get_text() # Popularity
  except:
    Popularity = None
  
  try:
    creator_part = bf.find_all('div', {'class': 'ipc-metadata-list-item__content-container'})
    # Director
    director_part = creator_part[0].find_all('a')
    director_list = []
    for director in director_part:
      director_list.append(director.get_text())
    # Writer
    writer_part = creator_part[1].find_all('a')
    writer_list = []
    for writer in writer_part:
      writer_list.append(writer.get_text())

    # Stars
    stars_part = creator_part[2].find_all('a')
    stars_list = []
    for stars in stars_part:
      stars_list.append(stars.get_text())
  except:
    director_list, writer_list, stars_list = [], [], []
    print(f'No creator part and current url is {film_url}')

  # Awards
  try:
    Awards = bf.find('section', {'cel_widget_id': 'StaticFeature_Awards', 'class': 'ipc-page-section ipc-page-section--base celwidget'})
    Awards = Awards.find('li', {'data-testid': 'award_information'}).find('label', {'class': 'ipc-metadata-list-item__list-content-item'}).get_text()
  except:
    Awards = None

  # Reviews_info
  Reviews_info = bf_info.find('ul', {'class': 'ipc-inline-list sc-124be030-0 ddUaJu baseAlt',
                                     'data-testid': 'reviewContent-all-reviews'})
  Reviews_info = Reviews_info.find_all('li')
  # User reviews
  try:
    User_reviews = Reviews_info[0].find('span', {'class': 'score'}).get_text()
  except:
    User_reviews = None
  # Critic reviews
  try:
    Critic_reviews = Reviews_info[1].find('span', {'class': 'score'}).get_text()
  except:
    Critic_reviews = None
  # Metascore
  try:
    Metascore = Reviews_info[2].find('span', {'class': 'score'}).get_text()
  except:
    Metascore = None


  # For Info Details
  try:
    details = bf.find('section',{'data-testid':'Details', 'class': 'ipc-page-section ipc-page-section--base celwidget'})
  except:
    details = None
    print(f'No details for {film_url}')
  # Release Date
  try:
    Release_date = details.find('li', {'role':'presentation', 'data-testid': 'title-details-releasedate'}).\
                   find('div').get_text()
  except:
    Release_date = None
  # Country of origin
  try:
    Country_of_origin = []
    tmp_list = details.find('li', {'role':'presentation', 'data-testid': 'title-details-origin'}).\
               find('div').find_all('li')
    for tmp in tmp_list:
      Country_of_origin.append(tmp.get_text())
  except:
    Country_of_origin = []
  # Language
  try:
    Language = []
    tmp_list = details.find('li', {'role':'presentation', 'data-testid': 'title-details-languages'}).\
               find('div').find_all('li')
    for tmp in tmp_list:
      Language.append(tmp.get_text())
  except:
    Language = []
  # Filming locations
  try:
    Filming_locations = details.find('li', {'role':'presentation', 'data-testid': 'title-details-filminglocations'}).\
                        find('div').get_text()
  except:
    Filming_locations = None
  # Production companies
  try:
    Production_companies = []
    tmp_list = details.find('li', {'role':'presentation', 'data-testid': 'title-details-companies'}).\
               find('div').find_all('li')
    for tmp in tmp_list:
      Production_companies.append(tmp.get_text())
  except:
    Production_companies = []
  
  # Box office
  try:
    box_office = bf.find('section',{'data-testid':'BoxOffice', 'class': 'ipc-page-section ipc-page-section--base celwidget'})
    box_office = box_office.find('div', {'data-testid':'title-boxoffice-section'})
  except:
    box_office = None
  # Budget
  try:
    Budget = box_office.find('li', {'data-testid': 'title-boxoffice-budget'}).find('div').find('span').get_text()
  except:
    Budget = None
  # Gross US & Canada
  try:
    Gross_US_Canada = box_office.find('li', {'data-testid': 'title-boxoffice-grossdomestic'}).find('div').find('span').get_text()
  except:
    Gross_US_Canada = None
  # Opening weekend US & Canada
  try:
    Opening_weekend = box_office.find('li', {'data-testid': 'title-boxoffice-openingweekenddomestic'}).find('div').find('span').get_text()
  except:
    Opening_weekend = None
  # Gross worldwide
  try:
    Gross_worldwide = box_office.find('li', {'data-testid': 'title-boxoffice-cumulativeworldwidegross'}).find('div').find('span').get_text()
  except:
    Gross_worldwide = None

  # Technical Specs
  try:
    Technical_specs = bf.find('section',{'data-testid':'TechSpecs', 'class': 'ipc-page-section ipc-page-section--base celwidget'})
    Technical_specs = Technical_specs.find('div', {'data-testid':'title-techspecs-section'})
  except:
    Technical_specs = None
    print(f'No Technical Specs for {film_url}')
  # Runtime
  try:
    Runtime = Technical_specs.find('li', {'data-testid': 'title-techspec_runtime'}).find('div').get_text()
  except:
    Runtime = None
  # Color
  try:
    Color = Technical_specs.find('li', {'data-testid': 'title-techspec_color'}).find('div').get_text()
  except:
    Color = None
  # Sound mix
  try:
    Sound_mix = []
    Sound_mix_list = Technical_specs.find('li', {'data-testid': 'title-techspec_soundmix'}).find('div').find_all('li')
    for sound in Sound_mix_list:
      Sound_mix.append(sound.get_text())
  except:
    Sound_mix = []
  # Aspect ratio
  try:
    Aspect_ratio = Technical_specs.find('li', {'data-testid': 'title-techspec_aspectratio'}).find('div').get_text()
  except:
    Aspect_ratio = None

  return {
      'film_name': film_name,
      'synopsis': story_line,
      'genre_list': genre_list,
      'publish_year': publish_year,
      'MPAA': MPAA,
      'Duration_minute': Duration_str,
      'Rating': Rating,
      'Rating_popularity': Rating_popularity,
      'Popularity': Popularity,
      'Director': director_list,
      'Writer': writer_list,
      'Stars': stars_list,
      'Awards': Awards,
      'User_reviews': User_reviews,
      'Critic_reviews': Critic_reviews,
      'Metascore': Metascore,
      'Release_date': Release_date,
      'Country_of_origin': Country_of_origin,
      'Language': Language,
      'Filming_locations': Filming_locations,
      'Production_companies': Production_companies,
      'Budget': Budget,
      'Gross_US_Canada': Gross_US_Canada,
      'Opening_weekend': Opening_weekend,
      'Gross_worldwide': Gross_worldwide,
      'Runtime': Runtime,
      'Color': Color,
      'Sound_mix': Sound_mix,
      'Aspect_ratio': Aspect_ratio,
      'film_url': film_url
  }

a = get_film_info('https://www.imdb.com/title/tt2366450/?ref_=kw_li_tt')
b = get_film_info('https://www.imdb.com/title/tt6341832/?ref_=kw_li_tt')

b

def scrap_film_url_list(start_year = 1992, end_year = 2022):
  '''
  Filter_rules: (sample size: 112,464)
  - Featured Film
  - IMDB RATING From 3 to 10
  - At least 10000 votes

  URL: https://www.imdb.com/search/keyword/?ref_=kw_ref_yr&mode=detail&page=&title_type=movie&user_rating=3.0%2C&num_votes=10000%2C&sort=moviemeter,asc&release_date=2022%2C2022

  start_year & end_year: int - the year starting or ending recording films
  Output: List[str] - the validate url list of films published between start_year and end_year
  '''
  page_url_list = []
  # Get the url of page by year
  for target_year in range(start_year, end_year + 1):
    URL = f'https://www.imdb.com/search/keyword/?ref_=kw_ref_yr&mode=detail&page=&title_type=movie&user_rating=3.0%2C&num_votes=10000%2C&sort=moviemeter,asc&release_date={target_year}%2C{target_year}'
    try:
      page_url_list.extend(get_all_list_page(URL))
    except:
      print(URL)
  print('Already get url of page by year')
  print('# of unique pages: ', len(page_url_list))

  # Parse the tag of film
  film_url_list = []
  for page_url in page_url_list: # url of one page
    page_film_tag = get_all_film_url(page_url) # The list of film url tag in one page
    for film_tag in page_film_tag: # One film tag in one page
      film_url = readable_film_url(back_tag = film_tag, front_tag = 'https://www.imdb.com') # Transfer to url
      film_url_list.append(film_url)
  print('# of unique movies: ', len(set(film_url_list)))
  return film_url_list

import pandas as pd
def scrap_film_info_by_url(film_url_list):
  '''
  Output: DataFrame with film information
  '''
  film_info_df = pd.DataFrame()
  for film_url in film_url_list:
    film_info = get_film_info(film_url)
    film_info_df = film_info_df.append(film_info, ignore_index=True)
  return film_info_df

film_url_list = scrap_film_url_list(start_year = 1900, end_year = 2022)

len(film_url_list)

from google.colab import drive
drive.mount('/content/drive')

pd.DataFrame(film_url_list, columns = ['url']).to_pickle('/content/drive/MyDrive/film_url_10158.pkl')

"""#### 2010-2022"""

film_url_list = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/film_url_10158.pkl')
film_url_list = list(film_url_list['url'])

len(set(film_url_list))

film_info_df = scrap_film_info_by_url(film_url_list)

r1 = get_film_info('https://www.imdb.com/title/tt2366450/?ref_=kw_li_tt')
r2 = get_film_info('https://www.imdb.com/title/tt11804152/?ref_=kw_li_tt')
r3 = get_film_info('https://www.imdb.com/title/tt10696896/?ref_=kw_li_tt')

# film_info_df.to_pickle('/content/drive/MyDrive/ISE 540/Project/film_info_df_v2.pkl')

df2 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/film_info_df_v2.pkl')
df2.iloc[6877, :] = r1
df2.iloc[9794, :] = r2
df2.iloc[9799, :] = r3

df2.to_pickle('/content/drive/MyDrive/ISE 540/Project/film_info_df_v3.pkl')

"""# Cleaning Process"""

def data_clean(df):
    def data_tans1(Duration_minute):
        if Duration_minute is None:
            return None
        a = Duration_minute.replace("h", '*60').replace(' ','+').replace('m','*1')
        return eval(a)

    def data_tans2(Rating_popularity):
        if Rating_popularity is None:
            return None
        a = Rating_popularity.replace("K","*1e3")
        a = a.replace("M","*1e6")
        return eval(a)

    def data_tans3(Release_date):
        if Release_date is None:
            return None
        a = Release_date[Release_date.find("(")+1:Release_date.find(")")]
        return a

   # def data_tans4(Country_of_origin):
    #    if Country_of_origin == 'nan' :
     #       return np.nan
      #  a = str(Country_of_origin).replace('United States', 'America')
       # a = a.replace('United Kingdom', 'Britain')
        #return a


    def data_tran5(Release_date):
        if Release_date is None:
            return None
        if len(Release_date.split(' ')) == 4:
            a = Release_date.split(' ')[-2] + '-' + data_dict[Release_date.split(' ')[0]] + '-' + Release_date.split(' ')[1].split(',')[0]
            a = datetime.datetime.strptime(a,'%Y-%m-%d')
            return a
        if len(Release_date.split(' ')) == 3:
            a = Release_date.split(' ')[1] + '-' + data_dict[Release_date.split(' ')[0]]
            a = datetime.datetime.strptime(a,'%Y-%m')
            return a
        if len(Release_date.split(' ')) == 2:
            a = Release_date.split(' ')[0]
            a = datetime.datetime.strptime(a,'%Y')
            return a
    data_dict = {'January':'01','February':'02','March':'03','April':'04','May':'05',
             'June':'06','July':'07','August':'08','September':'09','October':'10','November':'11','December':'12'}

    
        
    df['Duration_minute']=  df['Duration_minute'].apply(lambda x:data_tans1(x))
    df["Rating_popularity"] = df["Rating_popularity"].apply(lambda x:data_tans2(x))
    df[['Win', 'Nomination']] = df['Awards'].str.split('&', 1, expand=True)
    # seperate Awards into Win and Nomination
    df['Win'] = df['Win'].str.replace(r'\D', '')
    df['Nomination'] = df['Nomination'].str.replace(r'\D', '')
    df = df.drop('Awards',axis = 1)
    df['Release_location'] = df['Release_date'].apply(lambda st: data_tans3(st))
    #get the release location from release date 
    df['Release_date'] = df['Release_date'].str.replace(r"\(.*\)","")
    #drop release location from release date
   # df_language_list = df['Language'].tolist()
   # language_list = df_language_list
   # df['Country_of_origin'] = df['Country_of_origin'].apply(lambda x:data_tans4(x))
    
    #country
   # df_country_list = df['Country_of_origin'].tolist()
   # country_list = []
  #  for l in df_country_list:
   #     if l == 'nan':
    #        l = "None"
     #   split_list1 = re.findall('[A-Z][a-z]*',l)
      #  country_list.append(split_list1)
   # df['Country_of_origin'] = country_list
   # df['Language'] = language_list
    df['Filming_locations'] = df['Filming_locations'].str.rsplit(',').str[-1] 
    df['Budget'] = df['Budget'].str.replace('$', '')
    df['Budget'] = df['Budget'].str.replace(r"\(.*\)","")
    df['Gross_US_Canada'] = df['Gross_US_Canada'].str.replace('$', '')
    df['Opening_weekend'] = df['Opening_weekend'].str.replace('$', '')
    df['Gross_worldwide'] = df['Gross_worldwide'].str.replace('$', '')
    #delete $ and text in parentheses 
    df['Color'] = df['Color'].str.replace(
        'Black and White(original release)Black and White','Black and White').replace(
        'ColorBlack and White','Color, Black and White').replace(
        'Black and WhiteColor','Color, Black and White')
    
    df['Release_date'] = df['Release_date'].apply(lambda x:data_tran5(x))
    df['Rating_popularity'] = df['Rating_popularity'].apply(lambda x:int(x))
    df['Duration_minute'] = df['Duration_minute'].apply(lambda x:int(x))
    return df

"""## Execute"""

df1 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/film_info_df.pkl')
df2 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/film_info_df_v3.pkl')

# Budget	Gross_US_Canada Opening_weekend	Gross_worldwide
df2['Budget'] = df1['Budget']
df2['Gross_US_Canada'] = df1['Gross_US_Canada']
df2['Opening_weekend'] = df1['Opening_weekend']
df2['Gross_worldwide'] = df1['Gross_worldwide']

import datetime
df3 = data_clean(df2)
df3['Rating_popularity'] = df3['Rating_popularity'].astype('int')

pd.set_option('display.max_columns', None)
df3.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/film_info_df_v4.pkl')

df3

"""# LDA & TFIDF baseline"""

import re
import nltk
import tqdm
import matplotlib.pyplot as plt
from pprint import pprint
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag, ne_chunk
from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

from gensim import corpora, models
import gensim
from gensim.models import CoherenceModel

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.inspection import permutation_importance

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR


nltk.download(['punkt', 'wordnet'])
nltk.download('stopwords') # download for lemmatization
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('omw-1.4')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
df = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/film_with_allinfo.pkl')
df.drop(columns = ['director_keyword_vector', 'director_keyword'], axis = 1, inplace = True)
df.drop(columns = ['actor_keyword', 'actor_keyword_vector'], axis = 1, inplace = True)
df.drop(columns = ['Production_keyword', 'Production_keyword_vector'], axis = 1, inplace = True)
df.drop(columns = ['writer_keyword', 'writer_keyword_vector'], axis = 1, inplace = True)

"""## Tf-idf

## LDA

### Hyperparameter tuning
"""

def texts_for_LDA(doc_set):
  '''
  Input:  doc_set - the list of doc text
  Output: texts - the list of list for documental tokens
  '''
  texts = []
  for doc_text in doc_set:
    doc_token = text_to_token(doc_text)
    texts.append(doc_token)
  return texts

def text_to_token(text):
  text = re.sub(r"[^a-zA-Z0-9]", " ", text.lower())
  tokens = word_tokenize(text)
  tokens = [t.strip() for t in tokens if t not in stopwords.words("english")]
  is_noun = lambda pos: pos[:2] == 'NN'
  nouns = [word for word, pos in nltk.pos_tag(tokens) if is_noun(pos)]
  # nouns = [PorterStemmer().stem(n) for n in nouns]
  lemmatizer = WordNetLemmatizer()
  clean_tokens=[]
  for tok in nouns:
    clean_tok = lemmatizer.lemmatize(tok, pos='n').lower().strip()
    clean_tokens.append(clean_tok)
  return clean_tokens

# supporting function
'''
Text and dictionary are pre defined
'''
def compute_coherence_values(corpus, dictionary_id2, k, a, b):
    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=dictionary_id2,
                                           num_topics=k, random_state=100,
                                           chunksize=100,passes=10,
                                           alpha=a,eta=b)
    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
    return coherence_model_lda.get_coherence()

grid = {}
grid['Validation_Set'] = {}
# Topics range
topics_range = range(2, 11, 2)
# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.2))
# Beta parameter
beta = list(np.arange(0.01, 1, 0.2))
# Validation sets
# num_of_docs = len(corpus)
corpus_title = ['75% Corpus', '100% Corpus']

"""#### Synopsis"""

doc_set = df['synopsis'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# Baseline Model
lda_model = gensim.models.LdaMulticore(corpus = corpus, id2word = dictionary,
                                      num_topics = 10, random_state = 100,
                                      chunksize = 100, passes = 10,
                                      per_word_topics = True)

# pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': [],
                 'var_name': []}
# iterate through validation corpuses
for i in range(len(corpus_sets)):
  # iterate through number of topics
  for k in topics_range:
    # iterate through alpha values
    for a in alpha:
      # iterare through beta values
      for b in beta:
        # get the coherence score for the given parameters
        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary_id2=dictionary, k=k, a=a, b=b)
        # Save the model results
        model_results['Validation_Set'].append(corpus_title[i])
        model_results['Topics'].append(k)
        model_results['Alpha'].append(a)
        model_results['Beta'].append(b)
        model_results['Coherence'].append(cv)
        model_results['var_name'].append('Synopsis')
# pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project'
pd.DataFrame(model_results).to_csv(f'{share_file_path}/synopsis_lda_tuning_results.csv',
                                   index = False)

pd.DataFrame(model_results).sort_values(by = 'Coherence', ascending = False)

lda_model = gensim.models.LdaMulticore(corpus = corpus_sets[0], id2word = dictionary,
                                       num_topics = 8, random_state = 100,
                                       chunksize=100, passes = 10,
                                       alpha = 0.01, eta= 0.61)

pprint(lda_model.print_topics(num_words = 5))

"""#### Director"""

doc_set = df['directorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': [],
                 'var_name': []}

# iterate through validation corpuses
for i in range(len(corpus_sets)):
  # iterate through number of topics
  for k in topics_range:
    # iterate through alpha values
    for a in alpha:
      # iterare through beta values
      for b in beta:
        # get the coherence score for the given parameters
        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary_id2=dictionary, k=k, a=a, b=b)
        # Save the model results
        model_results['Validation_Set'].append(corpus_title[i])
        model_results['Topics'].append(k)
        model_results['Alpha'].append(a)
        model_results['Beta'].append(b)
        model_results['Coherence'].append(cv)
        model_results['var_name'].append('Director')

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
pd.DataFrame(model_results).to_csv(f'{share_file_path}/director_lda_tuning_results.csv',
                                   index = False)
pd.DataFrame(model_results).sort_values(by = 'Coherence', ascending = False)

"""#### Writer"""

doc_set = df['writerext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': [],
                 'var_name': []}

# iterate through validation corpuses
for i in range(len(corpus_sets)):
  # iterate through number of topics
  for k in topics_range:
    # iterate through alpha values
    for a in alpha:
      # iterare through beta values
      for b in beta:
        # get the coherence score for the given parameters
        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary_id2=dictionary, k=k, a=a, b=b)
        # Save the model results
        model_results['Validation_Set'].append(corpus_title[i])
        model_results['Topics'].append(k)
        model_results['Alpha'].append(a)
        model_results['Beta'].append(b)
        model_results['Coherence'].append(cv)
        model_results['var_name'].append('Writer')

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
pd.DataFrame(model_results).to_csv(f'{share_file_path}/writer_lda_tuning_results.csv',
                                   index = False)
pd.DataFrame(model_results).sort_values(by = 'Coherence', ascending = False)

"""#### Actor"""

doc_set = df['actorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': [],
                 'var_name': []}

# iterate through validation corpuses
for i in range(len(corpus_sets)):
  # iterate through number of topics
  for k in topics_range:
    # iterate through alpha values
    for a in alpha:
      # iterare through beta values
      for b in beta:
        # get the coherence score for the given parameters
        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary_id2=dictionary, k=k, a=a, b=b)
        # Save the model results
        model_results['Validation_Set'].append(corpus_title[i])
        model_results['Topics'].append(k)
        model_results['Alpha'].append(a)
        model_results['Beta'].append(b)
        model_results['Coherence'].append(cv)
        model_results['var_name'].append('Actor')

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
pd.DataFrame(model_results).to_csv(f'{share_file_path}/actor_lda_tuning_results.csv',
                                   index = False)
pd.DataFrame(model_results).sort_values(by = 'Coherence', ascending = False)

"""#### Production company"""

doc_set = df['productionext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': [],
                 'var_name': []}

# iterate through validation corpuses
for i in range(len(corpus_sets)):
  # iterate through number of topics
  for k in topics_range:
    # iterate through alpha values
    for a in alpha:
      # iterare through beta values
      for b in beta:
        # get the coherence score for the given parameters
        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary_id2=dictionary, k=k, a=a, b=b)
        # Save the model results
        model_results['Validation_Set'].append(corpus_title[i])
        model_results['Topics'].append(k)
        model_results['Alpha'].append(a)
        model_results['Beta'].append(b)
        model_results['Coherence'].append(cv)
        model_results['var_name'].append('Production')

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
pd.DataFrame(model_results).to_csv(f'{share_file_path}/production_lda_tuning_results.csv',
                                   index = False)
pd.DataFrame(model_results).sort_values(by = 'Coherence', ascending = False)

"""### Tuning Result"""

tune = pd.read_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_lda_tuning_results.csv')
tune['var_name'] = None
tune.iloc[:250, -1] = 'Synopsis'
tune.iloc[250:500, -1] = 'Director'
tune.iloc[500:750, -1] = 'Writer'
tune.iloc[750:1000,-1] = 'Actor'

share_file_path = '/content/drive/MyDrive/ISE 540/Project/Text/540Project'
tune.to_csv(f'{share_file_path}/summary_lda_tuning_results.csv', index = False)

def construct_lda_with_param(corpus, corpus_idx, k, a, b):
  num_of_docs = len(corpus)
  corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]
  lda_model = gensim.models.LdaMulticore(corpus=corpus_sets[corpus_idx],id2word = dictionary,
                                         num_topics = k, random_state=100,
                                         chunksize=100,passes=10,
                                         alpha = a, eta = b)
  return lda_model

"""#### Synopsis"""

tune[tune['var_name'] == 'Synopsis'].sort_values(by = 'Coherence', ascending = False)

"""#### Director"""

tune[tune['var_name'] == 'Director'].sort_values(by = 'Coherence', ascending = False)

"""Best model: 75% Corpus, 8 Topics, 0.21 Alpha, 0.81 Beta"""

doc_set = df['directorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# 75% Corpus, 8 Topics, 0.21 Alpha, 0.81 Beta
lda_model = construct_lda_with_param(0, 8, 0.21, 0.81)

pprint(lda_model.print_topics())

"""#### Writer"""

tune[tune['var_name'] == 'Writer'].sort_values(by = 'Coherence', ascending = False)

doc_set = df['writerext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# 100% Corpus, 10 Topics, 0.61 Alpha, 0.81 Beta
lda_model = construct_lda_with_param(1, 10, 0.61, 0.81)

pprint(lda_model.print_topics())

"""#### Actor"""

tune[tune['var_name'] == 'Actor'].sort_values(by = 'Coherence', ascending = False)

doc_set = df['actorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

# 75% Corpus, 8 Topics, 0.81 Alpha, 0.81 Beta
lda_model = construct_lda_with_param(0, 8, 0.81, 0.81)

pprint(lda_model.print_topics())

"""#### Production"""

doc_set = df['productionext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

share_file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
production = pd.read_csv(f'{share_file_path}/production_lda_tuning_results.csv')
production[production['Topics'] == 6].sort_values(by = 'Coherence', ascending = False).head()
# production.groupby('Topics')['Coherence'].mean()

# 75% Corpus, 8 Topics, 0.81 Alpha, 0.81 Beta
lda_model = construct_lda_with_param(corpus, 0, 6, 0.61, 0.01)

pprint(lda_model.print_topics(num_words = 20))

"""# Feature Engineering

## LDA
"""

def get_topics_score(doc_set, lda_model, topics_num):
  train_vecs = []
  for i in range(len(doc_set)):
      top_topics = (
          lda_model.get_document_topics(corpus[i], minimum_probability=0.0)
      )
      topic_vec = [top_topics[i][1] for i in range(topics_num)]
      # topic_vec.extend([rev_train.iloc[i].real_counts])
      # topic_vec.extend([len(rev_train.iloc[i].text)])
      train_vecs.append(topic_vec)
  return train_vecs

"""### Actor"""

actor_tune = pd.read_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_lda_tuning_results.csv')
tmp = actor_tune.groupby('Topics')['Coherence'].max().reset_index()
tmp

actor_tune[actor_tune['Topics'] == 8].sort_values(by = 'Coherence', ascending = False).head()

doc_set = df['actorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = construct_lda_with_param(corpus, 0, 8, 0.21, 0.81)
pprint(lda_model.print_topics(num_words = 30))

lda_actor = get_topics_score(doc_set, lda_model, 8)
lda_actor = pd.DataFrame(lda_actor, columns = ['actor_'+ str(i) for i in range(1,9)])
lda_actor.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_lda_score.csv', index = False)

"""### Director"""

director_tune = pd.read_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/director_lda_tuning_results.csv')
tmp = director_tune.groupby('Topics')['Coherence'].max().reset_index()
plt.plot(tmp['Topics'], tmp['Coherence'])

director_tune[director_tune['Topics'] == 6].sort_values(by = 'Coherence', ascending = False).head()

doc_set = df['directorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = construct_lda_with_param(corpus, 1, 6, 0.61, 0.01)
pprint(lda_model.print_topics(num_words = 30))

lda_director = get_topics_score(doc_set, lda_model, 6)
lda_director = pd.DataFrame(lda_director, columns = ['director_'+ str(i) for i in range(1,7)])
lda_director.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/director_lda_score.csv', index = False)

"""### Production"""

production_tune = pd.read_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/production_lda_tuning_results.csv')
tmp = production_tune.groupby('Topics')['Coherence'].max().reset_index()
plt.plot(tmp['Topics'], tmp['Coherence'])

production_tune[production_tune['Topics'] == 4].sort_values(by = 'Coherence', ascending = False).head()

doc_set = df['productionext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = construct_lda_with_param(corpus, 0, 4, 0.41, 0.41)
pprint(lda_model.print_topics(num_words = 30))

lda_production = get_topics_score(doc_set, lda_model, 4)
lda_production = pd.DataFrame(lda_production, columns = ['production_'+ str(i) for i in range(1,5)])
lda_production.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/production_lda_score.csv', index = False)

"""### Writer"""

writer_tune = pd.read_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/writer_lda_tuning_results.csv')
tmp = writer_tune.groupby('Topics')['Coherence'].max().reset_index()
plt.plot(tmp['Topics'], tmp['Coherence'])

writer_tune[writer_tune['Topics'] == 6].sort_values(by = 'Coherence', ascending = False).head()

doc_set = df['writerext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = construct_lda_with_param(corpus, 1, 6, 0.41, 0.81)
pprint(lda_model.print_topics(num_words = 30))

lda_writer = get_topics_score(doc_set, lda_model, 6)
lda_writer = pd.DataFrame(lda_writer, columns = ['writer_'+ str(i) for i in range(1,7)])
lda_writer.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/writer_lda_score.csv', index = False)

"""### merging together & processing"""

lda_actor['actor_topics'] = lda_actor.iloc[:,:-1].apply(lambda x: np.argmax(x) + 1, axis = 1)
lda_director['director_topics'] = lda_director.iloc[:,:-1].apply(lambda x: np.argmax(x) + 1, axis = 1)
lda_production['production_topics'] = lda_production.iloc[:,:-1].apply(lambda x: np.argmax(x) + 1, axis = 1)
lda_writer['writer_topics'] = lda_writer.iloc[:,:-1].apply(lambda x: np.argmax(x) + 1, axis = 1)

topics_feature = lda_actor.copy()
topics_feature = pd.merge(topics_feature, lda_director, left_index=True, right_index=True)
topics_feature = pd.merge(topics_feature, lda_production, left_index=True, right_index=True)
topics_feature = pd.merge(topics_feature, lda_writer, left_index=True, right_index=True)
topics_feature.info()

for col in topics_feature.columns:
  if topics_feature[col].dtype == 'int':
    topics_feature[col] = topics_feature[col].astype('int8')
  else:
    topics_feature[col] = topics_feature[col].astype('float16')

topics_feature.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/summary_lda_score.pkl')

"""## TF-IDF [Synopsis]"""

def noun_tokenize(text):
  """
  Function to tokenize text.
  """
  text = re.sub(r"[^a-zA-Z0-9]", " ", text.lower())
  tokens = word_tokenize(text)
  tokens = [t.strip() for t in tokens if t not in stopwords.words("english")]

  is_noun = lambda pos: pos[:2] == 'NN'
  nouns = [word for word, pos in nltk.pos_tag(tokens) if is_noun(pos)]

  # nouns = [PorterStemmer().stem(n) for n in nouns]
  lemmatizer = WordNetLemmatizer()
  clean_tokens=[]
  for tok in nouns:
    clean_tok = lemmatizer.lemmatize(tok, pos='n').lower().strip()
    clean_tokens.append(clean_tok)
  
  return clean_tokens

corpus = df['synopsis'].tolist()
vectorizer = TfidfVectorizer(tokenizer = noun_tokenize, 
                             max_features = 100,
                             ngram_range = (1, 3))
x = vectorizer.fit_transform(corpus)
df_tfidf = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())
col_list = df_tfidf.columns

df_tfidf.columns = ['story_'+str(col) for col in col_list]
for col in df_tfidf.columns:
  df_tfidf[col] = df_tfidf[col].astype('float16')

df_tfidf.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/synopsis_results.pkl')

"""## Getting text features"""

text_feature = pd.merge(topics_feature, df_tfidf, left_index=True, right_index=True)
text_feature

text_feature.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/text_feature.pkl')

"""### Actor"""

doc_set = df['actorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model_actor = construct_lda_with_param(corpus, 0, 8, 0.21, 0.81)

actor_topic1_keyword = [word for word, _ in lda_model_actor.show_topic(0, topn = 30)]
actor_topic2_keyword = [word for word, _ in lda_model_actor.show_topic(1, topn = 30)]
actor_topic3_keyword = [word for word, _ in lda_model_actor.show_topic(2, topn = 30)]
actor_topic4_keyword = [word for word, _ in lda_model_actor.show_topic(3, topn = 30)]
actor_topic5_keyword = [word for word, _ in lda_model_actor.show_topic(4, topn = 30)]
actor_topic6_keyword = [word for word, _ in lda_model_actor.show_topic(5, topn = 30)]
actor_topic7_keyword = [word for word, _ in lda_model_actor.show_topic(6, topn = 30)]
actor_topic8_keyword = [word for word, _ in lda_model_actor.show_topic(7, topn = 30)]

"""### Director"""

doc_set = df['directorext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model_director = construct_lda_with_param(corpus, 1, 6, 0.61, 0.01)

director_topic1_keyword = [word for word, _ in lda_model_director.show_topic(0, topn = 30)]
director_topic2_keyword = [word for word, _ in lda_model_director.show_topic(1, topn = 30)]
director_topic3_keyword = [word for word, _ in lda_model_director.show_topic(2, topn = 30)]
director_topic4_keyword = [word for word, _ in lda_model_director.show_topic(3, topn = 30)]
director_topic5_keyword = [word for word, _ in lda_model_director.show_topic(4, topn = 30)]
director_topic6_keyword = [word for word, _ in lda_model_director.show_topic(5, topn = 30)]

"""### Production"""

doc_set = df['productionext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model_production = construct_lda_with_param(corpus, 0, 4, 0.41, 0.41)

production_topic1_keyword = [word for word, _ in lda_model_production.show_topic(0, topn = 30)]
production_topic2_keyword = [word for word, _ in lda_model_production.show_topic(1, topn = 30)]
production_topic3_keyword = [word for word, _ in lda_model_production.show_topic(2, topn = 30)]
production_topic4_keyword = [word for word, _ in lda_model_production.show_topic(3, topn = 30)]

"""### Writer"""

doc_set = df['writerext'].to_list()
texts = texts_for_LDA(doc_set)
# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts) 
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model_writer = construct_lda_with_param(corpus, 1, 6, 0.41, 0.81)

writer_topic1_keyword = [word for word, _ in lda_model_writer.show_topic(0, topn = 30)]
writer_topic2_keyword = [word for word, _ in lda_model_writer.show_topic(1, topn = 30)]
writer_topic3_keyword = [word for word, _ in lda_model_writer.show_topic(2, topn = 30)]
writer_topic4_keyword = [word for word, _ in lda_model_writer.show_topic(3, topn = 30)]
writer_topic5_keyword = [word for word, _ in lda_model_writer.show_topic(4, topn = 30)]
writer_topic6_keyword = [word for word, _ in lda_model_writer.show_topic(5, topn = 30)]

"""### Merging and storage"""

dict_keyword = {
    'actor_topic1_keyword': actor_topic1_keyword,
    'actor_topic2_keyword': actor_topic2_keyword,
    'actor_topic3_keyword': actor_topic3_keyword,
    'actor_topic4_keyword': actor_topic4_keyword,
    'actor_topic5_keyword': actor_topic5_keyword,
    'actor_topic6_keyword': actor_topic6_keyword,
    'actor_topic7_keyword': actor_topic7_keyword,
    'actor_topic8_keyword': actor_topic8_keyword,
    'director_topic1_keyword': director_topic1_keyword,
    'director_topic2_keyword': director_topic2_keyword,
    'director_topic3_keyword': director_topic3_keyword,
    'director_topic4_keyword': director_topic4_keyword,
    'director_topic5_keyword': director_topic5_keyword,
    'director_topic6_keyword': director_topic6_keyword,
    'production_topic1_keyword': production_topic1_keyword,
    'production_topic2_keyword': production_topic2_keyword,
    'production_topic3_keyword': production_topic3_keyword,
    'production_topic4_keyword': production_topic4_keyword,
    'writer_topic1_keyword': writer_topic1_keyword,
    'writer_topic2_keyword': writer_topic2_keyword,
    'writer_topic3_keyword': writer_topic3_keyword,
    'writer_topic4_keyword': writer_topic4_keyword,
    'writer_topic5_keyword': writer_topic5_keyword,
    'writer_topic6_keyword': writer_topic6_keyword,
}
df_keyword = pd.DataFrame(dict_keyword)

df_keyword.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/topics_keywords.csv')

"""# Transformer"""

pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
# sentences = ["This is an example sentence", "Each sentence is converted"]
model = SentenceTransformer('sentence-transformers/nli-roberta-large')
# embeddings = model.encode(sentences)
# print(embeddings)

"""## Embedding for LDA

### Embedding topics
"""

def agg_embedding_vectors(embed_matrix):
  res_vec = []
  for j in range(len(embed_matrix[0])):
    res_val = 0
    for i in range(len(embed_matrix)):
      res_val += embed_matrix[i][j]
    res_vec.append(res_val / len(embed_matrix))
  return res_vec

# Actor
actor_topic1_embed = agg_embedding_vectors(model.encode(actor_topic1_keyword))
actor_topic2_embed = agg_embedding_vectors(model.encode(actor_topic2_keyword))
actor_topic3_embed = agg_embedding_vectors(model.encode(actor_topic3_keyword))
actor_topic4_embed = agg_embedding_vectors(model.encode(actor_topic4_keyword))
actor_topic5_embed = agg_embedding_vectors(model.encode(actor_topic5_keyword))
actor_topic6_embed = agg_embedding_vectors(model.encode(actor_topic6_keyword))
actor_topic7_embed = agg_embedding_vectors(model.encode(actor_topic7_keyword))
actor_topic8_embed = agg_embedding_vectors(model.encode(actor_topic8_keyword))

# Director
director_topic1_embed = agg_embedding_vectors(model.encode(director_topic1_keyword))
director_topic2_embed = agg_embedding_vectors(model.encode(director_topic2_keyword))
director_topic3_embed = agg_embedding_vectors(model.encode(director_topic3_keyword))
director_topic4_embed = agg_embedding_vectors(model.encode(director_topic4_keyword))
director_topic5_embed = agg_embedding_vectors(model.encode(director_topic5_keyword))
director_topic6_embed = agg_embedding_vectors(model.encode(director_topic6_keyword))

# Production
production_topic1_embed = agg_embedding_vectors(model.encode(production_topic1_keyword))
production_topic2_embed = agg_embedding_vectors(model.encode(production_topic2_keyword))
production_topic3_embed = agg_embedding_vectors(model.encode(production_topic3_keyword))
production_topic4_embed = agg_embedding_vectors(model.encode(production_topic4_keyword))

# Writer
writer_topic1_embed = agg_embedding_vectors(model.encode(writer_topic1_keyword))
writer_topic2_embed = agg_embedding_vectors(model.encode(writer_topic2_keyword))
writer_topic3_embed = agg_embedding_vectors(model.encode(writer_topic3_keyword))
writer_topic4_embed = agg_embedding_vectors(model.encode(writer_topic4_keyword))
writer_topic5_embed = agg_embedding_vectors(model.encode(writer_topic5_keyword))
writer_topic6_embed = agg_embedding_vectors(model.encode(writer_topic6_keyword))

dict_embed = {
    'actor_topic1_embed': actor_topic1_embed,
    'actor_topic2_embed': actor_topic2_embed,
    'actor_topic3_embed': actor_topic3_embed,
    'actor_topic4_embed': actor_topic4_embed,
    'actor_topic5_embed': actor_topic5_embed,
    'actor_topic6_embed': actor_topic6_embed,
    'actor_topic7_embed': actor_topic7_embed,
    'actor_topic8_embed': actor_topic8_embed,
    'director_topic1_embed': director_topic1_embed,
    'director_topic2_embed': director_topic2_embed,
    'director_topic3_embed': director_topic3_embed,
    'director_topic4_embed': director_topic4_embed,
    'director_topic5_embed': director_topic5_embed,
    'director_topic6_embed': director_topic6_embed,
    'production_topic1_embed': production_topic1_embed,
    'production_topic2_embed': production_topic2_embed,
    'production_topic3_embed': production_topic3_embed,
    'production_topic4_embed': production_topic4_embed,
    'writer_topic1_embed': writer_topic1_embed,
    'writer_topic2_embed': writer_topic2_embed,
    'writer_topic3_embed': writer_topic3_embed,
    'writer_topic4_embed': writer_topic4_embed,
    'writer_topic5_embed': writer_topic5_embed,
    'writer_topic6_embed': writer_topic6_embed,
}
df_embed = pd.DataFrame(dict_embed)

df_embed.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/topics_embedding.csv')

"""### Transfer embedding score to docs"""

df_embed

# Transfer topics with embedding vector
tmp = topics_feature[['actor_topics', 'director_topics', 'production_topics','writer_topics']]
tmp['actor_embed'] = tmp['actor_topics'].apply(lambda x: 
                                          df_embed[f'actor_topic{x}_embed'].to_list())
tmp['director_embed'] = tmp['director_topics'].apply(lambda x: 
                                          df_embed[f'director_topic{x}_embed'].to_list())
tmp['production_embed'] = tmp['production_topics'].apply(lambda x: 
                                          df_embed[f'production_topic{x}_embed'].to_list())
tmp['writer_embed'] = tmp['writer_topics'].apply(lambda x: 
                                          df_embed[f'writer_topic{x}_embed'].to_list())

tmp.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/embedding_matrix.pkl')

"""### Sentence Embedding"""

actor_sent = list(df['actorext'])
director_sent = list(df['directorext'])
production_sent = list(df['productionext'])
writer_sent = list(df['writerext'])

writer_sent_embed = model.encode(writer_sent)
pd.DataFrame(writer_sent_embed).to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/writer_sentence_embedding.pkl')

actor_sent_embed = model.encode(actor_sent)
director_sent_embed = model.encode(director_sent)
production_sent_embed = model.encode(production_sent)

pd.DataFrame(actor_sent_embed).to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_sentence_embedding.pkl')
pd.DataFrame(director_sent_embed).to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/director_sentence_embedding.pkl')
pd.DataFrame(production_sent_embed).to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/production_sentence_embedding.pkl')

file_path = '/content/drive/MyDrive/ISE 540/Project/540Project/Text'
with open(f'{file_path}/actor_text.txt', 'w') as f:
    for line in actor_sent:
        f.write(f"{line}\n")

with open(f'{file_path}/actor_text.txt') as f:
    lines = f.readlines()

tmp = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/text_feature.pkl')

drop_list = []
for col in tmp.columns:
  if 'topics' in col:
    drop_list.append(col)

tmp.drop(columns = drop_list, axis = 1, inplace = True)

tmp.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/text_feature_v2.pkl')

tmp = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/final.pkl')
tmp.info()

tmp

"""# Predictor

## Get top model
"""

df = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/final.pkl')
df2 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/text_feature_v2.pkl')
df3 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/wiki_embedding.pkl')
story_col = []
non_story = []
for col in df2.columns:
  if 'story' in col:
    story_col.append(col)
  else:
    non_story.append(col)
story_tfidf = df2[story_col]
lda_similarity = df2[non_story]
df_basis = df.copy()
df_tfidf = pd.merge(df_basis, story_tfidf, left_index = True, right_index = True)
df_lda = pd.merge(df_basis, lda_similarity, left_index = True, right_index = True)
df_lda_tfidf = pd.merge(df_lda, story_tfidf, left_index = True, right_index = True)
df_wiki = pd.merge(df_basis, df3, left_index = True, right_index = True)
df_lda_wiki = pd.merge(df_lda, df3, left_index = True, right_index = True)
df_wiki_tfidf = pd.merge(df_wiki, story_tfidf, left_index = True, right_index = True)
df_wiki_tfidf_lda = pd.merge(df_wiki_tfidf, lda_similarity, left_index = True, right_index = True)

def split_x_y(df):
  y = df['Rating']
  X = df.drop(columns=['Rating'], axis = 1)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
  return X_train, X_test, y_train, y_test

def get_error_metrics(model,X_test,y_test):
  y_pred=model.predict(X_test)
  MSE=mean_squared_error(y_test, y_pred)
  MAE=mean_absolute_error(y_test, y_pred)
  return MSE,MAE

def build_fit_model(model_name, X_train, y_train):
  if model_name == 'GradientBoost':
    reg = GradientBoostingRegressor(random_state=42)
  elif model_name == 'AdaBoost':
    reg = AdaBoostRegressor(random_state=42)#, n_estimators=100
  elif model_name == 'RandomForest':
    reg = RandomForestRegressor(random_state=42)
  elif model_name == 'ElasticNet':
    reg = Ridge() # random_state = 42
  elif model_name == 'LinearRegression':
    reg = LinearRegression()
  else:
    reg = SVR(C=100, gamma=0.1, epsilon=0.1)
  
  reg.fit(X_train, y_train)
  return reg

def get_regressor_error(df, model_name):
  X_train, X_test, y_train, y_test = split_x_y(df)
  model = build_fit_model(model_name, X_train, y_train)
  return get_error_metrics(model, X_test, y_test)

gb_params = GradientBoostingRegressor().get_params()
ad_params = AdaBoostRegressor().get_params()
rf_params = RandomForestRegressor().get_params()
en_params = ElasticNet().get_params()
lr_params = LinearRegression().get_params()
sv_params = SVR(C=100, gamma=0.1, epsilon=0.1).get_params()

def get_params_dict(x):
  if x == 'GradientBoost':
    return gb_params
  elif x == 'AdaBoost':
    return ad_params
  elif x == 'RandomForest':
    return rf_params
  elif x == 'ElasticNet':
    return en_params
  elif x == 'LinearRegression':
    return lr_params
  else:
    return sv_params

df_list = [df_basis, df_tfidf, df_lda, df_lda_tfidf, df_wiki, df_lda_wiki, df_wiki_tfidf, df_wiki_tfidf_lda]
df_name = ['Basis', 
           'Basis + TF-IDF',
           'Basis + LDA',
           'Basis + LDA + TF-IDF',
           'Basis + WIKI',
           'Basis + LDA + WIKI',
           'Basis + TF-IDF + WIKI',
           'Basis + LDA + TF-IDF + WIKI']
model_option = ['GradientBoost', 'AdaBoost', 'RandomForest', 'ElasticNet', 'LinearRegression', 'SVR']
dict_df = {
    'Regressor Name': [],
    'Attributes Set': [],
    'Mean Absolute Error': [],
    'Mean Square Error': []
}

i = -1
for df in df_list:
  i += 1
  for model_name in model_option:
    MSE, MAE = get_regressor_error(df, model_name)
    dict_df['Regressor Name'].append(model_name)
    dict_df['Attributes Set'].append(df_name[i])
    dict_df['Mean Absolute Error'].append(MAE)
    dict_df['Mean Square Error'].append(MSE)
    print(model_name, '', df_name[i])
# pd.DataFrame(dict_df).to_csv('/content/drive/MyDrive/ISE 540/Project/model_result.csv', index = False)

tmp.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/model_comparsion_v1.csv', index = False)

tmp[tmp['Regressor Name'] == 'LinearRegression'].reset_index(drop = True)

"""## Hyperparamter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, make_scorer

def split_x_y(df):
  y = df['Rating']
  X = df.drop(columns=['Rating'], axis = 1)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
  return X_train, X_test, y_train, y_test

def get_error_metrics(model,X_test,y_test):
  y_pred=model.predict(X_test)
  MSE=mean_squared_error(y_test, y_pred)
  MAE=mean_absolute_error(y_test, y_pred)
  return MSE,MAE

df = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/final.pkl')
df2 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/text_feature_v2.pkl')
df3 = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/wiki_embedding.pkl')
story_col = []
non_story = []
for col in df2.columns:
  if 'story' in col:
    story_col.append(col)
  else:
    non_story.append(col)
story_tfidf = df2[story_col]
lda_similarity = df2[non_story]
df_basis = df.copy()
df_tfidf = pd.merge(df_basis, story_tfidf, left_index = True, right_index = True)
df_lda = pd.merge(df_basis, lda_similarity, left_index = True, right_index = True)
df_lda_tfidf = pd.merge(df_lda, story_tfidf, left_index = True, right_index = True)
df_wiki = pd.merge(df_basis, df3, left_index = True, right_index = True)
df_lda_wiki = pd.merge(df_lda, df3, left_index = True, right_index = True)
df_wiki_tfidf = pd.merge(df_wiki, story_tfidf, left_index = True, right_index = True)
df_wiki_tfidf_lda = pd.merge(df_wiki_tfidf, lda_similarity, left_index = True, right_index = True)

gradientboost_parameters = {
  'learning_rate': [0.05, 0.1, 0.2, 0.5],
  'n_estimators': [50, 100, 200],
  'criterion': ['friedman_mse', 'mse'],
  'min_samples_split': [2, 5, 10],
  'max_depth': [3, 5]
}

def find_best_param(model, X_train, y_train, parameters):
  # 5-fold stratified cross validation wwith accuracy scoring
  np.random.seed(42) # The retain the same results for each run time
  cv = GridSearchCV(model, param_grid = parameters, scoring = 'neg_mean_squared_error', cv=5) 
  # When cv is integer, to specify the number of folds in a (Stratified)KFold
  return cv.fit(X_train, y_train)

X_train, X_test, y_train, y_test = split_x_y(df_lda)
model = GradientBoostingRegressor(random_state = 42)
grid_model_1 = find_best_param(model, X_train, y_train, gradientboost_parameters)

X_train, X_test, y_train, y_test = split_x_y(df_lda_tfidf)
model = GradientBoostingRegressor(random_state = 42)
grid_model_2 = find_best_param(model, X_train, y_train, gradientboost_parameters)

column_pre = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score','split0_test_score', 
              'split1_test_score', 'split2_test_score', 'split3_test_score','split4_test_score']
column_new = ['params', 'mean_valid_score', 'std_valid_score', 'rank_valid_score','split0_valid_score', 
              'split1_valid_score', 'split2_valid_score', 'split3_valid_score','split4_valid_score']

gb = pd.DataFrame(grid_model_1.cv_results_)[column_pre]
gb.columns = column_new
gb = gb.sort_values('rank_valid_score').reset_index(drop = True)
gb.head(10)

gb1 = pd.DataFrame(grid_model_2.cv_results_)[column_pre]
gb1.columns = column_new
gb1 = gb1.sort_values('rank_valid_score').reset_index(drop = True)
gb1.head(10)

col_1 = ['GradientBoost' for _ in range(144)]
col_2 = ['Basis+LDA' for _ in range(144)]
col_3 = ['Basis+LDA+TF-IDF' for _ in range(144)]
dict_1 = {
    'Regressor Name': col_1,
    'Attribute Set': col_2
}
dict_2 = {
    'Regressor Name': col_1,
    'Attribute Set': col_3
}
cv_gb = pd.merge(pd.DataFrame(dict_1), gb, left_index = True, right_index = True).append(pd.merge(pd.DataFrame(dict_2), gb1, left_index = True, right_index = True)).reset_index(drop=True)

cv_gb = cv_gb.sort_values(by = 'mean_valid_score', ascending = False).reset_index(drop=True)
cv_gb

cv_gb.to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/model_rank_gb.csv')

randomforest_parameters = {
  'n_estimators': [50, 100, 200],
  # 'criterion': ['squared_error', 'absolute_error'],
  'max_depth': [3, 5, 7],
  # 'max_features': ['sqrt', 'log2', 'auto'],
  'min_samples_split': [2, 5, 10],
}

X_train, X_test, y_train, y_test = split_x_y(df_lda)
model = RandomForestRegressor(random_state = 42)
grid_model_3 = find_best_param(model, X_train, y_train, randomforest_parameters)

X_train, X_test, y_train, y_test = split_x_y(df_lda_tfidf)
model = RandomForestRegressor(random_state = 42)
grid_model_4 = find_best_param(model, X_train, y_train, randomforest_parameters)

rf = pd.DataFrame(grid_model_3.cv_results_)[column_pre]
rf.columns = column_new
rf = rf.sort_values('rank_valid_score').reset_index(drop = True)
rf.head(10)

rf1 = pd.DataFrame(grid_model_4.cv_results_)[column_pre]
rf1.columns = column_new
rf1 = rf1.sort_values('rank_valid_score').reset_index(drop = True)
rf1.head(10)

col_1 = ['Random Forest' for _ in range(144)]
col_2 = ['Basis+LDA' for _ in range(144)]
col_3 = ['Basis+LDA+TF-IDF' for _ in range(144)]
dict_1 = {
    'Regressor Name': col_1,
    'Attribute Set': col_2
}
dict_2 = {
    'Regressor Name': col_1,
    'Attribute Set': col_3
}
cv_rf = pd.merge(pd.DataFrame(dict_1), rf, left_index = True, right_index = True).append(pd.merge(pd.DataFrame(dict_2), rf1, left_index = True, right_index = True)).reset_index(drop=True)

cv_rf = cv_rf.sort_values(by = 'mean_valid_score', ascending = False).reset_index(drop=True)
cv_rf

# cv = cv_gb.append(cv_rf)
cv_rf.sort_values(by = 'mean_valid_score', ascending = False).reset_index(drop=True).iloc[0]['params']

df_lda['Year'] = df_lda['Year'].astype('int32')

X_train, X_test, y_train, y_test = split_x_y(df_lda)
model = RandomForestRegressor(random_state = 42, max_depth = 7, min_samples_split = 5, n_estimators = 200)
model.fit(X_train, y_train)

pip install shap
import shap

y_predict = model.predict(X_test)
# Fits the explainer
explainer = shap.Explainer(model.predict, X_test)
# Calculates the SHAP values - It takes some time
shap_values = explainer(X_test)

shap_v = shap_values.values
shap_v

plt.figure(figsize = (12, 6))
shap.plots.bar(shap_values)

plt.figure(figsize = (12, 8))
shap.plots.beeswarm(shap_values)

# shap.plots.waterfall(shap_values[0])
shap.plots.bar(shap_values[0])

shap.summary_plot(shap_values, plot_type='violin')

y_predict = model.predict(X_test)
MSE = mean_squared_error(y_predict, y_test)
MAE = mean_absolute_error(y_predict, y_test)
r_square = r2_score(y_predict, y_test)

MSE

MAE

r_square

X_test

df.iloc[6855,:]

diff = abs(y_test - y_predict)
diff.sort_values()

y_index = list(y_test.index)

y_predict[y_index.index(5758)]

"""# Other"""

actor_sent = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_sentence_embedding.pkl')
director_sent = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/director_sentence_embedding.pkl')
production_sent = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/production_sentence_embedding.pkl')
writer_sent = pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/writer_sentence_embedding.pkl')

actor_sent.columns = ['actor_embed' + str(col) for col in actor_sent.columns]
director_sent.columns = ['director_embed' + str(col) for col in director_sent.columns]
production_sent.columns = ['production_embed' + str(col) for col in production_sent.columns]
writer_sent.columns = ['writer_embed' + str(col) for col in writer_sent.columns]

wiki_embedding = actor_sent.copy()
wiki_embedding = pd.merge(wiki_embedding, director_sent, left_index = True, right_index = True)
wiki_embedding = pd.merge(wiki_embedding, production_sent, left_index = True, right_index = True)
wiki_embedding = pd.merge(wiki_embedding, writer_sent, left_index = True, right_index = True)

wiki_embedding.to_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/wiki_embedding.pkl')

actor_phrase = []
with open('/content/drive/MyDrive/ISE 540/Project/540Project/Text/Autophrase/actor_text/AutoPhrase_multi-words.txt') as f:
    lines = f.readlines()
for word in lines:
  actor_phrase.append(word.split('\t')[1].replace('\n', ''))

director_phrase = []
with open('/content/drive/MyDrive/ISE 540/Project/540Project/Text/Autophrase/director_text/AutoPhrase_multi-words.txt') as f:
    lines = f.readlines()
for word in lines:
  director_phrase.append(word.split('\t')[1].replace('\n', ''))

production_phrase = []
with open('/content/drive/MyDrive/ISE 540/Project/540Project/Text/Autophrase/production_text/AutoPhrase_multi-words.txt') as f:
    lines = f.readlines()
for word in lines:
  production_phrase.append(word.split('\t')[1].replace('\n', ''))

writer_phrase = []
with open('/content/drive/MyDrive/ISE 540/Project/540Project/Text/Autophrase/writer_text/AutoPhrase_multi-words.txt') as f:
    lines = f.readlines()
for word in lines:
  writer_phrase.append(word.split('\t')[1].replace('\n', ''))

actor_phrase = actor_phrase[:200]
director_phrase = director_phrase[:200]
production_phrase = production_phrase[:200]
writer_phrase = writer_phrase[:200]

dict_df = {
    'actor_phrase': actor_phrase,
    'director_phrase': director_phrase,
    'production_phrase': production_phrase,
    'writer_phrase': writer_phrase
}

pd.DataFrame(dict_df).to_csv('/content/drive/MyDrive/ISE 540/Project/540Project/Text/Autophrase/wiki_autophrase.csv', index = False)

pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/wiki_embedding.pkl')

pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/synopsis_results.pkl')

import pandas as pd
pd.read_pickle('/content/drive/MyDrive/ISE 540/Project/540Project/Text/actor_sentence_embedding.pkl')

